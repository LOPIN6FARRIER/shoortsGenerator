import OpenAI from "openai";
import { CONFIG } from "./config.js";
import { Logger } from "./utils.js";

type ProviderName = "ollama" | "openai";

interface LLMProvider {
  name: ProviderName;
  client: OpenAI;
  model: string;
}

// Cache por provider para manejar fallbacks
let lastFailedProvider: ProviderName | null = null;
let lastFailTime = 0;
const RETRY_FAILED_PROVIDER_AFTER = 5 * 60 * 1000; // Reintentar provider fallido despu√©s de 5 min

/**
 * üîç Detecta si Ollama est√° disponible en el sistema
 */
async function isOllamaAvailable(): Promise<boolean> {
  try {
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), 2000);

    const baseUrl = CONFIG.ollama.baseUrl.replace(/\/v1\/?$/, "");
    const response = await fetch(`${baseUrl}/api/tags`, {
      signal: controller.signal,
    });

    clearTimeout(timeoutId);
    return response.ok;
  } catch (error) {
    return false;
  }
}

/**
 * üîç Verifica si OpenAI est√° configurado
 */
function isOpenAIConfigured(): boolean {
  return !!CONFIG.openai.apiKey;
}

/**
 * ü§ñ Obtiene el cliente LLM con sistema de fallback
 *
 * Orden de prioridad:
 * 1. Ollama (local, sin l√≠mites)
 * 2. OpenAI (de pago, fallback)
 */
export async function getLLMClient(): Promise<LLMProvider> {
  // Si el provider fallido ya pas√≥ el tiempo de retry, reseteamos
  if (
    lastFailedProvider &&
    Date.now() - lastFailTime > RETRY_FAILED_PROVIDER_AFTER
  ) {
    Logger.info(
      `‚ôªÔ∏è  Reintentando provider ${lastFailedProvider} despu√©s de cooldown`,
    );
    lastFailedProvider = null;
  }

  // Intentar Ollama primero
  const ollamaAvailable = await isOllamaAvailable();
  if (
    ollamaAvailable &&
    CONFIG.ollama.enabled &&
    lastFailedProvider !== "ollama"
  ) {
    Logger.info(`ü¶ô Usando Ollama (${CONFIG.ollama.model}) - Local LLM`);
    return {
      name: "ollama",
      client: new OpenAI({
        baseURL: CONFIG.ollama.baseUrl,
        apiKey: "ollama",
      }),
      model: CONFIG.ollama.model,
    };
  }

  // Fallback: OpenAI
  if (isOpenAIConfigured()) {
    Logger.info(`ü§ñ Usando OpenAI (${CONFIG.openai.model}) - Fallback de pago`);
    return {
      name: "openai",
      client: new OpenAI({ apiKey: CONFIG.openai.apiKey }),
      model: CONFIG.openai.model,
    };
  }

  throw new Error(
    "‚ùå No hay ning√∫n LLM disponible. Configura OPENAI_API_KEY o instala Ollama.",
  );
}

/**
 * üéØ Obtiene el modelo del provider
 */
export function getModel(provider: LLMProvider): string {
  return provider.model;
}

/**
 * ‚ùå Marca un provider como fallido (para activar fallback)
 */
export function markProviderFailed(providerName: ProviderName): void {
  lastFailedProvider = providerName;
  lastFailTime = Date.now();
  Logger.warn(
    `‚ö†Ô∏è  Provider ${providerName} marcado como fallido, usando fallback...`,
  );
}

/**
 * üîÑ Fuerza recarga del provider en la pr√≥xima llamada
 */
export function resetLLMCache(): void {
  lastFailedProvider = null;
  lastFailTime = 0;
  Logger.info("‚ôªÔ∏è  Cache de LLM reseteado");
}
